\subsection{SGD Convergence for Convex and Smooth Functions}
\noindent \textbf{Theorem.} Let us say that we have a function $f$ that is both a Sum of $L-$Smooth Functions and a Sum of Convex Functions. Let us say that the sequence of iterates generated by the SGD Algorithm is $(x^{(t)})_{t \in \mathbb{N}}$ with a sequence of step sizes that satisfy $0 < \gamma_t < \frac{1}{4L_{max}}$. 

Let us denote $\bar{x}^T = \frac{1}{\sum_{t = 0}^{T - 1} \gamma_t} \sum_{t = 0}^{T - 1} \gamma_t x^t$ \newline 

$\mathbb{E}[f(\bar{x}^T) - \inf f] \leq \frac{||x^{(0)} - x^*||^2}{\sum_{t = 0}^{T - 1} \gamma_t} + 2\sigma_f^* \frac{\sum_{t = 0}^{T - 1} \gamma_t^2}{\sum_{t = 0}^{T - 1} \gamma_t}$

\noindent \textbf{Proof:} \newline 
Let us have $x^* \in \arg \min f$. We have already showed that when $f$ is a Sum of Convex functions, $\sigma_f^* = \mathbb{V}[\nabla f_i(x^*)]$. 

We know that, in Stochastic Gradient Descent, our iterates progress as such: $x^{(t + 1)}  = x^{(t)} - \gamma_t \nabla f_{i_t}(x^{(t)})$ \newline 

$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - \gamma_t \nabla f_{i_t}(x^{(t - 1)}) - x^*||^2$ \newline 

$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^* - \gamma_t \nabla f_{i_t}(x^{(t - 1)})||^2$ \newline 

$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\langle x^{(t - 1)} - x^*,  \gamma_t \nabla f_{i_t}(x^{(t - 1)})\rangle + ||\gamma_t \nabla f_{i_t}(x^{(t - 1)})||^2$ \newline 

$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\langle x^{(t - 1)} - x^*,  \gamma_t \nabla f_{i_t}(x^{(t - 1)})\rangle + \gamma_t^2 ||\nabla f_{i_t}(x^{(t - 1)})||^2$ \newline 

$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\gamma_t \langle x^{(t - 1)} - x^*,  \nabla f_{i_t}(x^{(t - 1)})\rangle + \gamma_t^2 ||\nabla f_{i_t}(x^{(t - 1)})||^2$ \newline 

Now, let's take the Expectation conditioned on $x^{(t - 1)}$ \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\gamma_t \langle x^{(t - 1)} - x^*,  \nabla f(x^{(t - 1)})\rangle + \gamma_t^2 \mathbb{E}||\nabla f_{i_t}(x^{(t - 1)})||^2$ \newline 

Based on the definition of convexity, we know that $f(y) \geq f(x) + \nabla(f(x))^T (y - x)$ \newline 

This would mean that $f(x^*) \geq f(x^{(t - 1)}) + \nabla(f(x^{(t - 1)}))^T (x^* - x^{(t - 1)})$ \newline 


$f(x^*) \geq f(x^{(t - 1)}) + \nabla(f(x^{(t - 1)}))^T (x^* - x^{(t - 1)})$ \newline 


$\nabla(f(x^{(t - 1)}))^T (x^{(t - 1)} - x^*) \geq f(x^{(t - 1)}) - f(x^*)$ \newline 

We can substitute this into the earlier equation we derived and get: \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\gamma_t \langle x^{(t - 1)} - x^*,  \nabla f(x^{(t - 1)})\rangle + \gamma_t^2 \mathbb{E}||\nabla f_{i_t}(x^{(t - 1)})||^2 \leq ||x^{(t - 1)} - x^*||^2 - 2\gamma_t (f(x^{(t - 1)}) - f(x^*)) + \gamma_t^2 \mathbb{E}||\nabla f_{i_t}(x^{(t - 1)})||^2$ \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 \leq ||x^{(t - 1)} - x^*||^2 - 2\gamma_t (f(x^{(t - 1)}) - f(x^*)) + \gamma_t^2 \mathbb{E}||\nabla f_{i_t}(x^{(t - 1)})||^2$ \newline 


Earlier, we proved that, when we have a function that is a sum of $L-$ Smooth functions and that is a sum of convex functions, $\mathbb{E}[||\nabla f_i(x)||^2] \leq 4L_{max} (f(x) - \inf f) + 2 \sigma_f^*$ \newline 

We can substitute this into the equations we derived: \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 \leq ||x^{(t - 1)} - x^*||^2 - 2\gamma_t (f(x^{(t - 1)}) - f(x^*)) + \gamma_t^2 (4L_{max} (f(x) - \inf f) + 2 \sigma_f^*) $ \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 \leq ||x^{(t - 1)} - x^*||^2 - 2\gamma_t (f(x^{(t - 1)}) - f(x^*)) + \gamma_t^2 4L_{max} (f(x) - \inf f) + 2\gamma_t^2 \sigma_f^* $ \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 \leq ||x^{(t - 1)} - x^*||^2 + (2 \gamma_t) (2 \gamma_t L_{max} - 1)(f(x^{(t - 1)}) - f(x^*)) + 2\gamma_t^2 \sigma_f^* $ \newline 

Since $\gamma_t < \frac{1}{4L_{max}}$, $2\gamma_t L_{max} - 1 < \frac{-1}{2}$. We also know that $(f(x^{(t - 1)}) - f(x^*)) > 0$ Hence \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 \leq ||x^{(t - 1)} - x^*||^2 - \gamma_t (f(x^{(t - 1)}) - f(x^*)) + 2\gamma_t^2 \sigma_f^* $ \newline 

Once again, let's take expectation over both sides of this inequality \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 \leq \mathbb{E} ||x^{(t - 1)} - x^*||^2 - \gamma_t \mathbb{E} (f(x^{(t - 1)}) - f(x^*)) + 2\gamma_t^2 \sigma_f^* $ \newline 

$\gamma_t \mathbb{E} (f(x^{(t - 1)}) - f(x^*)) \leq \mathbb{E} ||x^{(t - 1)} - x^*||^2 - \mathbb{E}||x^{(t)} - x^*||^2 + 2\gamma_t^2 \sigma_f^* $ \newline 

$\gamma_t \mathbb{E} (f(x^{(t - 1)}) - \inf f) \leq \mathbb{E} ||x^{(t - 1)} - x^*||^2 - \mathbb{E}||x^{(t)} - x^*||^2 + 2\gamma_t^2 \sigma_f^* $ \newline 

Let's build this up recursively: \newline 
$\gamma_1 \mathbb{E} (f(x^{(0)}) - \inf f) \leq \mathbb{E} ||x^{(0)} - x^*||^2 - \mathbb{E}||x^{(1)} - x^*||^2 + 2\gamma_1^2 \sigma_f^* $ \newline 


$\gamma_2 \mathbb{E} (f(x^{(1)}) - \inf f) \leq \mathbb{E} ||x^{(1)} - x^*||^2 - \mathbb{E}||x^{(2)} - x^*||^2 + 2\gamma_2^2 \sigma_f^* $ \newline 

$\gamma_3 \mathbb{E} (f(x^{(2)}) - \inf f) \leq \mathbb{E} ||x^{(2)} - x^*||^2 - \mathbb{E}||x^{(3)} - x^*||^2 + 2\gamma_3^2 \sigma_f^* $ \newline 

$\sum_{t=1}^{T - 1} \gamma_t \mathbb{E} (f(x^{(t - 1)}) - f(x^*)) = \mathbb{E} ||x^{(0)} - x^*||^2 - \mathbb{E}||x^{(T)} - x^*||^2 +  \sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_f^*$

We know that $\mathbb{E}||x^{(T)} - x^*||^2 > 0$. Hence, we can work with this inequality as such: \newline 

$\sum_{t=1}^{T - 1} \gamma_t \mathbb{E} (f(x^{(t - 1)}) - f(x^*)) = \mathbb{E} ||x^{(0)} - x^*||^2 - \mathbb{E}||x^{(T)} - x^*||^2 +  \sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_f^* \leq \mathbb{E} ||x^{(0)} - x^*||^2 + +  \sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_f^*$ \newline 

$\sum_{t=1}^{T - 1} \gamma_t \mathbb{E} (f(x^{(t - 1)}) - f(x^*)) \leq \mathbb{E} ||x^{(0)} - x^*||^2 + \sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_f^*$ \newline 

$\sum_{t=1}^{T - 1} \gamma_t \mathbb{E} (f(x^{(t - 1)}) - f(x^*)) \leq  ||x^{(0)} - x^*||^2 +  \sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_f^*$ \newline 

Let's divide both sides of this inequality by $\sum_{t = 1}^{T - 1} \gamma_t$ \newline 


$\mathbb{E} [\sum_{t=1}^{T - 1} \frac{\gamma_t}{\sum_{t = 1}^{T - 1} \gamma_t}  (f(x^{(t - 1)}) - f(x^*))] \leq \frac{||x^{(0)} - x^*||^2}{\sum_{t = 1}^{T - 1} \gamma_t} + \frac{\sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_f^*}{\sum_{t = 1}^{T - 1} \gamma_t}$ \newline 

$\mathbb{E} [\sum_{t=1}^{T - 1} \frac{1}{\sum_{t = 1}^{T - 1} \gamma_t}  (\gamma_t f(x^{(t - 1)}) - \gamma_t f(x^*))] \leq \frac{||x^{(0)} - x^*||^2}{\sum_{t = 1}^{T - 1} \gamma_t} + \frac{\sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_f^*}{\sum_{t = 1}^{T - 1} \gamma_t}$ \newline 


We know that $f$ is convex. Hence, we can apply the Generalized Jensen's Inequality. 

Based on the Generalized Jensen's Inequality, we can see that: \newline 

$f(\bar{x}^T) \leq \frac{1}{\sum_{t = 1}^{T - 1} \gamma_t} \sum_{t=1}^{T - 1} y_t f(x^{(t - 1)})$

Hence, our inequality becomes \newline 

$\mathbb{E} [f(\bar{x}^T) - \inf f] \leq \mathbb{E} [\sum_{t=1}^{T - 1} \frac{1}{\sum_{t = 1}^{T - 1} \gamma_t}  (\gamma_t f(x^{(t - 1)}) - \gamma_t f(x^*))] \leq \frac{||x^{(0)} - x^*||^2}{\sum_{t = 1}^{T - 1} \gamma_t} + \frac{\sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_f^*}{\sum_{t = 1}^{T - 1} \gamma_t}$ \newline 

$\mathbb{E} [f(\bar{x}^T) - \inf f] \leq \frac{||x^{(0)} - x^*||^2}{\sum_{t = 1}^{T - 1} \gamma_t} + \frac{\sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_f^*}{\sum_{t = 1}^{T - 1} \gamma_t}$ \newline 

\noindent \textbf{Theorem.} Let us say that we have a function $f$ that is both a Sum of $L-$Smooth Functions and a Sum of Convex Functions. Let us say that the sequence of iterates generated by the SGD Algorithm is $(x^{(t)})_{t \in \mathbb{N}}$ with a constant step size $\gamma_t = \gamma \leq \frac{1}{4L_{max}}$.

Let us denote $\bar{x}^T = \frac{1}{\sum_{t = 0}^{T - 1} \gamma_t} \sum_{t = 0}^{T - 1} \gamma_t x^t = \frac{1}{\gamma T} \gamma \sum_{t = 0}^{T - 1} x^t = \frac{1}{T} \sum_{t = 0}^{T - 1} x^t$ \newline 

Then for every $T \geq 1$ \newline 

$\mathbb{E}[f(\bar{x}^T) - \inf f] \leq \frac{||x^{(0)} - x^*||^2}{\gamma T} + 2\gamma \sigma_f^* $ \newline 

\noindent \textbf{Proof.} The proof of this is very simple as we did the majority of heavylifting in the last proof. We know that $\sum_{t = 0}^{T - 1} \gamma_t = \gamma T$ and $\sum_{t = 0}^{T - 1} \gamma_t^2 = T\gamma^2$. Let us substitute this into the last theorem and we will proceed from there. 

$\mathbb{E} [f(\bar{x}^T) - \inf f] \leq \frac{||x^{(0)} - x^*||^2}{\sum_{t = 1}^{T - 1} \gamma_t} + \frac{\sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_f^*}{\sum_{t = 1}^{T - 1} \gamma_t}$ \newline

$\mathbb{E} [f(\bar{x}^T) - \inf f] \leq \frac{||x^{(0)} - x^*||^2}{\gamma T} + \frac{2 \sigma_f^* T\gamma^2}{\gamma T}$ \newline

$\mathbb{E} [f(\bar{x}^T) - \inf f] \leq \frac{||x^{(0)} - x^*||^2}{\gamma T} + 2 \sigma_f^* \gamma$ \newline

\noindent \textbf{Theorem.} Let us say that we have a function $f$ that is both a Sum of $L-$Smooth Functions and a Sum of Convex Functions. Let us say that the sequence of iterates generated by the SGD Algorithm is $(x^{(t)})_{t \in \mathbb{N}}$ with a vanishing step size $\gamma_t = \frac{\gamma_0}{\sqrt{t + 1}}$ where $\gamma_0 \leq \frac{1}{4L_{max}}$ 

Let us denote $\bar{x}^T = \frac{1}{\sum_{t = 0}^{T - 1} \gamma_t} \sum_{t = 0}^{T - 1} \gamma_t x^t$ \newline 

Then for every $T \geq 1$ \newline 

$\mathbb{E}[f(\bar{x}^T) - \inf f] \leq \frac{5 ||x^{(0)} - x^*||^2}{4 \gamma_0 \sqrt{T}} + \sigma_f^* \frac{5 \gamma_0 \log{(T + 1)}}{\sqrt{T}} = \mathbb{O}(\frac{\log{(T + 1)}}{\sqrt{T}})$ \newline 

\noindent \textbf{Proof.} We know that our stepsize is decreasing. Hence, we can clearly see that $\gamma_t \leq \gamma_0 \leq \frac{1}{4L_{max}}$ for $t \geq 0$. Hence, we can apply the earlier result that we derived: \newline 

$\mathbb{E}[f(\bar{x}^T) - \inf f] \leq \frac{||x^{(0)} - x^*||^2}{\sum_{t = 0}^{T - 1} \gamma_t} + 2\sigma_f^* \frac{\sum_{t = 0}^{T - 1} \gamma_t^2}{\sum_{t = 0}^{T - 1} \gamma_t}$ \newline 

Based on the Sum-Integral Bounds, we can say that $\sum_{t = 0}^{T - 1} \gamma_t = \gamma_0 \sum_{t = 1}^{T} \frac{1}{\sqrt{t}} \geq \frac{4\gamma_0}{5} \sqrt{T}$ \newline 

$\sum_{t = 0}^{T - 1} \gamma_t^2 = \gamma_0^2 \sum_{t = 1}^{T} \frac{1}{t} \leq 2 \gamma_0^2 \log(T + 1)$ \newline

Substituting it into the expected value, we get: \newline 

$\mathbb{E}[f(\bar{x}^T) - \inf f] \leq \frac{5 ||x^{(0)} - x^*||^2}{4 \gamma_0 \sqrt{T}} + \sigma_f^* \frac{5 \gamma_0 \log{(T + 1)}}{\sqrt{T}} = \mathbb{O}(\frac{\log{(T + 1)}}{\sqrt{T}})$ \newline 


%MOVE THIS TO APPENDIX LATER%%%%%
Using the Sum-Integral Bounds, We will take a brief aside to discuss some theory that will be helpful to prove the theorem.  \newline 

Let's say we have a function $f: \mathbb{R}_{++} \rightarrow \mathbb{R}_{++}$ that is decreasing. \newline 

It is clear to see that $\int_{1}^{T + 1} f(x) \,dx \leq \sum_{x=1}^{T} f(x) = f(1) +  \sum_{x=2}^{T} f(x) \leq f(1) + \int_{1}^{T} f(x) \,dx$ \newline 

$\int_{1}^{T + 1} f(x) \,dx \leq \sum_{x=1}^{T} f(x) \leq f(1) + \int_{1}^{T} f(x) \,dx$ \newline 


Let's now use the functions $f(x) = \frac{1}{\sqrt{x}}$ and $f(x) = \frac{1}{x}$

Let's start with the function $f(x) = \frac{1}{\sqrt{x}}$ \newline 

$\int_{1}^{T + 1} \frac{1}{\sqrt{x}} \,dx \leq \sum_{x=1}^{T} \frac{1}{\sqrt{x}} \leq 1 + \int_{1}^{T} \frac{1}{\sqrt{x}} \,dx$ \newline 

Let's start by working towards the lower bound \newline 
$\int_{1}^{T + 1} \frac{1}{\sqrt{x}} \,dx = [2 \sqrt{x}]_1^{T + 1}  = 2 \sqrt{T + 1} - 2 = 2(\sqrt{T + 1} - 1)$ \newline 


We know that $\inf_{T \geq 1} \frac{\sqrt{T + 1} - 1}{\sqrt{T}} = \sqrt{2} - 1 > \frac{2}{5}$ \newline 

$\int_{1}^{T + 1} \frac{1}{\sqrt{x}} \,dx = [2 \sqrt{x}]_1^{T + 1}  = 2 \sqrt{T + 1} - 2 = 2(\sqrt{T + 1} - 1) \geq \frac{4}{5} \sqrt{T}$ \newline 

Now let's look at the upper bound: \newline 
$ 1 + \int_{1}^{T} \frac{1}{\sqrt{x}} \,dx = 1 + 2 \sqrt{T + 1} - 2  = 2 \sqrt{T + 1} - 1 $

Combining both the Lower and Upper Bounds gives us: \newline 


$\frac{4}{5} \sqrt{T} \leq \sum_{x=1}^{T} \frac{1}{\sqrt{x}} \leq 2 \sqrt{T + 1} - 1 $ \newline 

Now let's analyze the other function $f(x) = \frac{1}{x}$ \newline 

$\int_{1}^{T + 1} \frac{1}{x} \,dx \leq \sum_{x=1}^{T} \frac{1}{x} \leq 1 + \int_{1}^{T} \frac{1}{x} \,dx$ \newline 

Let's start by working towards the lower bound \newline 
$\int_{1}^{T + 1} \frac{1}{x} \,dx = [\log(t)]_1^{T + 1} = \log(T + 1)$ \newline 

Now let's look at the upper bound: \newline 
$1 + \int_{1}^{T} \frac{1}{x} \,dx = 1 + [\log(t)]_1^{T} = 1 + \log(T) \leq 2 \log(T + 1)$

Note: We know that $\sup_{T \geq 1} \frac{1 + \log(T)}{\log(T + 1)} \approx \sqrt{2} < 2$ \newline

$\log(T + 1) \leq \sum_{x=1}^{T} \frac{1}{x} \leq 2 \log(T + 1)$ \newline 



\subsection{SGD Convergence for Strongly Convex and Smooth Functions}
\noindent \textbf{Theorem.} Let us say that we have a function $f$ that is both a Sum of $L-$Smooth Functions and a Sum of Convex Functions. We will also assume that $f$ is $p$ strongly convex. Let us say that the sequence of iterates generated by the SGD Algorithm is $(x^{(t)})_{t \in \mathbb{N}}$ and we will assume that we have a constant stepsize satisfying $0 < \gamma < \frac{1}{2L_{max}}$. Then, we can say that for each iteration(i.e. $t \geq 0$) 
\begin{equation}
    \mathbb{E}||x^{(t)} - x^*||^2 \leq (1 - \gamma p)^t ||x^{(0)} - x^*||^2 + \frac{2 \gamma}{p} \sigma_f^*
\end{equation}

\noindent \textbf{Proof:} \newline 
We know that, in Stochastic Gradient Descent, our iterates progress as such: $x^{(t + 1)}  = x^{(t)} - \gamma \nabla f_{i_t}(x^{(t)})$ \newline 

$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - \gamma \nabla f_{i_t}(x^{(t - 1)}) - x^*||^2$ \newline 

$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^* - \gamma \nabla f_{i_t}(x^{(t - 1)})||^2$ \newline 


$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\langle x^{(t - 1)} - x^*,  \gamma \nabla f_{i_t}(x^{(t - 1)})\rangle + ||\gamma \nabla f_{i_t}(x^{(t - 1)})||^2$

$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\langle x^{(t - 1)} - x^*,  \gamma \nabla f_{i_t}(x^{(t - 1)})\rangle + \gamma^2 ||\nabla f_{i_t}(x^{(t - 1)})||^2$

Now, let's take the Expectation conditioned on $x^{(t - 1)}$ \newline 
$\mathbb{E}||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\gamma \langle x^{(t - 1)} - x^*,  \nabla f(x^{(t - 1)})\rangle + \gamma^2 \mathbb{E} ||\nabla f_{i_t}(x^{(t - 1)})||^2$ \newline 


Based on the definition of strong convexity, we know that $f(y) \geq f(x) + \nabla(f(x))^T (y - x) + \frac{p}{2} ||y - x||^2_2$ \newline 


This would mean that $f(x^*) \geq f(x^{(t - 1)}) + \nabla(f(x^{(t - 1)}))^T (x^* - x^{(t - 1)}) + \frac{p}{2} ||x^* - x^{(t - 1)}||^2_2$ \newline 


$f(x^*) \geq f(x^{(t - 1)}) + \nabla(f(x^{(t - 1)}))^T (x^* - x^{(t - 1)}) + \frac{p}{2} ||x^* - x^{(t - 1)}||^2_2$ \newline 


$\nabla(f(x^{(t - 1)}))^T (x^{(t - 1)} - x^*) \geq f(x^{(t - 1)}) - f(x^*) + \frac{p}{2} ||x^{(t - 1)} - x^*||^2_2$ \newline 

We can substitute this into the earlier equation we derived and get: \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\gamma \langle x^{(t - 1)} - x^*,  \nabla f(x^{(t - 1)})\rangle + \gamma^2 \mathbb{E} ||\nabla f_{i_t}(x^{(t - 1)})||^2 \leq ||x^{(t - 1)} - x^*||^2 - 2\gamma (f(x^{(t - 1)}) - f(x^*) + \frac{p}{2} ||x^{(t - 1)} - x^*||^2_2) + \gamma^2 \mathbb{E} ||\nabla f_{i_t}(x^{(t - 1)})||^2$ \newline 

Let's try to simplify $||x^{(t - 1)} - x^*||^2 - 2\gamma (f(x^{(t - 1)}) - f(x^*) + \frac{p}{2} ||x^{(t - 1)} - x^*||^2_2) + \gamma^2 \mathbb{E} ||\nabla f_{i_t}(x^{(t - 1)})||^2$ \newline 

$||x^{(t - 1)} - x^*||^2 - 2\gamma (f(x^{(t - 1)}) - f(x^*)) - p \gamma ||x^{(t - 1)} - x^*||^2_2 + \gamma^2 \mathbb{E} ||\nabla f_{i_t}(x^{(t - 1)})||^2$ \newline 


$(1 - p \gamma)||x^{(t - 1)} - x^*||^2 - 2\gamma (f(x^{(t - 1)}) - f(x^*))+ \gamma^2 \mathbb{E} ||\nabla f_{i_t}(x^{(t - 1)})||^2$ \newline 

$\mathbb{E}||x^{(t)} - x^*||^2  \leq (1 - p \gamma)||x^{(t - 1)} - x^*||^2 - 2\gamma (f(x^{(t - 1)}) - f(x^*))+ \gamma^2 \mathbb{E} ||\nabla f_{i_t}(x^{(t - 1)})||^2$ \newline 

Earlier, we proved that, when we have a function that is a sum of $L-$ Smooth functions and that is a sum of convex functions, $\mathbb{E}[||\nabla f_i(x)||^2] \leq 4L_{max} (f(x) - \inf f) + 2 \sigma_f^*$ \newline 

We can continue on with our proof as such: \newline 

$\mathbb{E}||x^{(t)} - x^*||^2  \leq (1 - p \gamma) \mathbb{E}||x^{(t - 1)} - x^*||^2 - 2\gamma(f(x^{(t - 1)}) - f(x^*)) + \gamma^2 (4L_{max} (f(x^{(t - 1)}) - \inf f) + 2 \sigma_f^*)$ \newline 

$\mathbb{E}||x^{(t)} - x^*||^2  \leq (1 - p \gamma) \mathbb{E}||x^{(t - 1)} - x^*||^2 + (2 \gamma) (2 \gamma L_{max} - 1)(f(x^{(t - 1)}) - f(x^*)) + 2\gamma^2 \sigma_f^*$ \newline 

$\mathbb{E}||x^{(t)} - x^*||^2  \leq (1 - p \gamma) \mathbb{E}||x^{(t - 1)} - x^*||^2 + (2 \gamma) (2 \gamma L_{max} - 1) \mathbb{E}(f(x^{(t - 1)}) - f(x^*)) + 2\gamma^2 \sigma_f^*$ \newline 

Since $\gamma < \frac{1}{2L_{max}}$, $2\gamma L_{max} - 1 < 0$. We also know that $\mathbb{E}(f(x^{(t - 1)}) - f(x^*)) > 0$ Hence \newline 

$\mathbb{E}||x^{(t)} - x^*||^2  \leq (1 - p \gamma) \mathbb{E}||x^{(t - 1)} - x^*||^2 + (2 \gamma) (2 \gamma L_{max} - 1) \mathbb{E}(f(x^{(t - 1)}) - f(x^*)) + 2\gamma^2 \sigma_f^* \leq (1 - p \gamma) \mathbb{E}||x^{(t - 1)} - x^*||^2 + 2\gamma^2 \sigma_f^*$ \newline 


$\mathbb{E}||x^{(t)} - x^*||^2  \leq (1 - p \gamma) \mathbb{E}||x^{(t - 1)} - x^*||^2 + 2\gamma^2 \sigma_f^*$ \newline 

Let's build this inequality recursively and see how it unfolds: \newline 
$\mathbb{E}||x^{(1)} - x^*||^2  \leq (1 - p \gamma) ||x^{(0)} - x^*||^2 + 2\gamma^2 \sigma_f^*$ \newline 

$\mathbb{E}||x^{(2)} - x^*||^2  \leq (1 - p \gamma) \mathbb{E}||x^{(1)} - x^*||^2 + 2\gamma^2 \sigma_f^*$ \newline 

$\mathbb{E}||x^{(2)} - x^*||^2  \leq (1 - p \gamma) ((1 - p \gamma) ||x^{(0)} - x^*||^2 + 2\gamma^2 \sigma_f^*) + 2\gamma^2 \sigma_f^*$ \newline 


We can see that $\mathbb{E}||x^{(t)} - x^*||^2 \leq (1 - \gamma p)^t ||x^{(0)} - x^*||^2 + \sum_{n=0}^{t - 1} (1 - p\gamma)^n 2\gamma^2 \sigma_f^*$

Let's look at the term $\sum_{n=0}^{t - 1} (1 - p\gamma)^n 2\gamma^2 \sigma_f^*$. 

$\sum_{n=0}^{t - 1} (1 - p\gamma)^n 2\gamma^2 \sigma_f^* < \sum_{n=0}^{\infty} (1 - p\gamma)^n 2\gamma^2 \sigma_f^* = \frac{1}{p\gamma} 2\gamma^2 \sigma_f^* = \frac{2\gamma \sigma_f^*}{p}$

Hence, we can see that \newline 
$\mathbb{E}||x^{(t)} - x^*||^2 \leq (1 - \gamma p)^t ||x^{(0)} - x^*||^2 + \sum_{n=0}^{t - 1} (1 - p\gamma)^n 2\gamma^2 \sigma_f^* \leq (1 - \gamma p)^t ||x^{(0)} - x^*||^2 + \frac{2 \gamma}{p} \sigma_f^*$

$\mathbb{E}||x^{(t)} - x^*||^2 \leq (1 - \gamma p)^t ||x^{(0)} - x^*||^2 + \frac{2 \gamma}{p} \sigma_f^*$

\subsection{Minibatch SGD Convergence for Convex and Smooth Functions}
\noindent \textbf{Theorem.} Let us say that we have a function $f$ that is both a Sum of $L-$Smooth Functions and a Sum of Convex Functions. Let us say that the sequence of iterates generated by the Minibatch SGD Algorithm is $(x^{(t)})_{t \in \mathbb{N}}$ with a sequence of step sizes that satisfy $0 < \gamma_t < \frac{1}{4L_{b}}$. 

Let us denote $\bar{x}^T = \frac{1}{\sum_{t = 0}^{T - 1} \gamma_t} \sum_{t = 0}^{T - 1} \gamma_t x^t$ \newline 

$\mathbb{E}[f(\bar{x}^T) - \inf f] \leq \frac{||x^{(0)} - x^*||^2}{\sum_{t = 0}^{T - 1} \gamma_t} + 2\sigma_b^* \frac{\sum_{t = 0}^{T - 1} \gamma_t^2}{\sum_{t = 0}^{T - 1} \gamma_t}$

\noindent \textbf{Proof:} \newline 
Let us have $x^* \in \arg \min f$. We have already showed that when $f$ is a Sum of Convex functions, $\sigma_b^* = \mathbb{V}[\nabla f_B(x^*)]$. 

We know that, in Minibatch Stochastic Gradient Descent, our iterates progress as such: $x^{(t + 1)}  = x^{(t)} - \gamma_t \nabla f_{B_t}(x^{(t)})$ \newline 

$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - \gamma_t \nabla f_{B_t}(x^{(t - 1)}) - x^*||^2$ \newline 

$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^* - \gamma_t \nabla f_{B_t}(x^{(t - 1)})||^2$ \newline 

$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\langle x^{(t - 1)} - x^*,  \gamma_t \nabla f_{B_t}(x^{(t - 1)})\rangle + ||\gamma_t \nabla f_{B_t}(x^{(t - 1)})||^2$ \newline 

$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\langle x^{(t - 1)} - x^*,  \gamma_t \nabla f_{B_t}(x^{(t - 1)})\rangle + \gamma_t^2 ||\nabla f_{B_t}(x^{(t - 1)})||^2$ \newline 

$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\gamma_t \langle x^{(t - 1)} - x^*,  \nabla f_{B_t}(x^{(t - 1)})\rangle + \gamma_t^2 ||\nabla f_{B_t}(x^{(t - 1)})||^2$ \newline 

Now, let's take the Expectation conditioned on $x^{(t - 1)}$ \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\gamma_t \langle x^{(t - 1)} - x^*,  \nabla f(x^{(t - 1)})\rangle + \gamma_t^2 \mathbb{E}||\nabla f_{B_t}(x^{(t - 1)})||^2$ \newline 

Based on the definition of convexity, we know that $f(y) \geq f(x) + \nabla(f(x))^T (y - x)$ \newline 

This would mean that $f(x^*) \geq f(x^{(t - 1)}) + \nabla(f(x^{(t - 1)}))^T (x^* - x^{(t - 1)})$ \newline 


$f(x^*) \geq f(x^{(t - 1)}) + \nabla(f(x^{(t - 1)}))^T (x^* - x^{(t - 1)})$ \newline 


$\nabla(f(x^{(t - 1)}))^T (x^{(t - 1)} - x^*) \geq f(x^{(t - 1)}) - f(x^*)$ \newline 

We can substitute this into the earlier equation we derived and get: \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\gamma_t \langle x^{(t - 1)} - x^*,  \nabla f(x^{(t - 1)})\rangle + \gamma_t^2 \mathbb{E}||\nabla f_{B_t}(x^{(t - 1)})||^2 \leq ||x^{(t - 1)} - x^*||^2 - 2\gamma_t (f(x^{(t - 1)}) - f(x^*)) + \gamma_t^2 \mathbb{E}||\nabla f_{B_t}(x^{(t - 1)})||^2$ \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 \leq ||x^{(t - 1)} - x^*||^2 - 2\gamma_t (f(x^{(t - 1)}) - f(x^*)) + \gamma_t^2 \mathbb{E}||\nabla f_{B_t}(x^{(t - 1)})||^2$ \newline 


Earlier, we proved that, when we have a function that is a sum of $L-$ Smooth functions and that is a sum of convex functions, $\mathbb{E}[||\nabla f_B(x)||^2] \leq 4L_{b} (f(x) - \inf f) + 2 \sigma_b^*$ \newline 

We can substitute this into the equations we derived: \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 \leq ||x^{(t - 1)} - x^*||^2 - 2\gamma_t (f(x^{(t - 1)}) - f(x^*)) + \gamma_t^2 (4L_{b} (f(x) - \inf f) + 2 \sigma_b^*) $ \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 \leq ||x^{(t - 1)} - x^*||^2 - 2\gamma_t (f(x^{(t - 1)}) - f(x^*)) + \gamma_t^2 4L_{b} (f(x) - \inf f) + 2\gamma_t^2 \sigma_b^* $ \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 \leq ||x^{(t - 1)} - x^*||^2 + (2 \gamma_t) (2 \gamma_t L_{b} - 1)(f(x^{(t - 1)}) - f(x^*)) + 2\gamma_t^2 \sigma_b^* $ \newline 

Since $\gamma_t < \frac{1}{4L_{b}}$, $2\gamma_t L_{b} - 1 < \frac{-1}{2}$. We also know that $(f(x^{(t - 1)}) - f(x^*)) > 0$ Hence \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 \leq ||x^{(t - 1)} - x^*||^2 - \gamma_t (f(x^{(t - 1)}) - f(x^*)) + 2\gamma_t^2 \sigma_b^* $ \newline 

Once again, let's take expectation over both sides of this inequality \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 \leq \mathbb{E} ||x^{(t - 1)} - x^*||^2 - \gamma_t \mathbb{E} (f(x^{(t - 1)}) - f(x^*)) + 2\gamma_t^2 \sigma_b^* $ \newline 

$\gamma_t \mathbb{E} (f(x^{(t - 1)}) - f(x^*)) \leq \mathbb{E} ||x^{(t - 1)} - x^*||^2 - \mathbb{E}||x^{(t)} - x^*||^2 + 2\gamma_t^2 \sigma_b^* $ \newline 

$\gamma_t \mathbb{E} (f(x^{(t - 1)}) - \inf f) \leq \mathbb{E} ||x^{(t - 1)} - x^*||^2 - \mathbb{E}||x^{(t)} - x^*||^2 + 2\gamma_t^2 \sigma_b^* $ \newline 

Let's build this up recursively: \newline 
$\gamma_1 \mathbb{E} (f(x^{(0)}) - \inf f) \leq \mathbb{E} ||x^{(0)} - x^*||^2 - \mathbb{E}||x^{(1)} - x^*||^2 + 2\gamma_1^2 \sigma_b^* $ \newline 


$\gamma_2 \mathbb{E} (f(x^{(1)}) - \inf f) \leq \mathbb{E} ||x^{(1)} - x^*||^2 - \mathbb{E}||x^{(2)} - x^*||^2 + 2\gamma_2^2 \sigma_b^* $ \newline 

$\gamma_3 \mathbb{E} (f(x^{(2)}) - \inf f) \leq \mathbb{E} ||x^{(2)} - x^*||^2 - \mathbb{E}||x^{(3)} - x^*||^2 + 2\gamma_3^2 \sigma_b^* $ \newline 

$\sum_{t=1}^{T - 1} \gamma_t \mathbb{E} (f(x^{(t - 1)}) - f(x^*)) = \mathbb{E} ||x^{(0)} - x^*||^2 - \mathbb{E}||x^{(T)} - x^*||^2 +  \sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_b^*$

We know that $\mathbb{E}||x^{(T)} - x^*||^2 > 0$. Hence, we can work with this inequality as such: \newline 

$\sum_{t=1}^{T - 1} \gamma_t \mathbb{E} (f(x^{(t - 1)}) - f(x^*)) = \mathbb{E} ||x^{(0)} - x^*||^2 - \mathbb{E}||x^{(T)} - x^*||^2 +  \sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_b^* \leq \mathbb{E} ||x^{(0)} - x^*||^2 + +  \sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_b^*$ \newline 

$\sum_{t=1}^{T - 1} \gamma_t \mathbb{E} (f(x^{(t - 1)}) - f(x^*)) \leq \mathbb{E} ||x^{(0)} - x^*||^2 + \sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_b^*$ \newline 

$\sum_{t=1}^{T - 1} \gamma_t \mathbb{E} (f(x^{(t - 1)}) - f(x^*)) \leq  ||x^{(0)} - x^*||^2 +  \sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_b^*$ \newline 

Let's divide both sides of this inequality by $\sum_{t = 1}^{T - 1} \gamma_t$ \newline 


$\mathbb{E} [\sum_{t=1}^{T - 1} \frac{\gamma_t}{\sum_{t = 1}^{T - 1} \gamma_t}  (f(x^{(t - 1)}) - f(x^*))] \leq \frac{||x^{(0)} - x^*||^2}{\sum_{t = 1}^{T - 1} \gamma_t} + \frac{\sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_b^*}{\sum_{t = 1}^{T - 1} \gamma_t}$ \newline 

$\mathbb{E} [\sum_{t=1}^{T - 1} \frac{1}{\sum_{t = 1}^{T - 1} \gamma_t}  (\gamma_t f(x^{(t - 1)}) - \gamma_t f(x^*))] \leq \frac{||x^{(0)} - x^*||^2}{\sum_{t = 1}^{T - 1} \gamma_t} + \frac{\sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_b^*}{\sum_{t = 1}^{T - 1} \gamma_t}$ \newline 


We know that $f$ is convex. Hence, we can apply the Generalized Jensen's Inequality. 

Based on the Generalized Jensen's Inequality, we can see that: \newline 

$f(\bar{x}^T) \leq \frac{1}{\sum_{t = 1}^{T - 1} \gamma_t} \sum_{t=1}^{T - 1} y_t f(x^{(t - 1)})$

Hence, our inequality becomes \newline 

$\mathbb{E} [f(\bar{x}^T) - \inf f] \leq \mathbb{E} [\sum_{t=1}^{T - 1} \frac{1}{\sum_{t = 1}^{T - 1} \gamma_t}  (\gamma_t f(x^{(t - 1)}) - \gamma_t f(x^*))] \leq \frac{||x^{(0)} - x^*||^2}{\sum_{t = 1}^{T - 1} \gamma_t} + \frac{\sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_b^*}{\sum_{t = 1}^{T - 1} \gamma_t}$ \newline 

$\mathbb{E} [f(\bar{x}^T) - \inf f] \leq \frac{||x^{(0)} - x^*||^2}{\sum_{t = 1}^{T - 1} \gamma_t} + \frac{\sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_b^*}{\sum_{t = 1}^{T - 1} \gamma_t}$ \newline 

\noindent \textbf{Theorem.} Let us say that we have a function $f$ that is both a Sum of $L-$Smooth Functions and a Sum of Convex Functions. Let us say that the sequence of iterates generated by the SGD Algorithm is $(x^{(t)})_{t \in \mathbb{N}}$ with a constant step size $\gamma_t = \gamma \leq \frac{1}{4L_{b}}$.

Let us denote $\bar{x}^T = \frac{1}{\sum_{t = 0}^{T - 1} \gamma_t} \sum_{t = 0}^{T - 1} \gamma_t x^t = \frac{1}{\gamma T} \gamma \sum_{t = 0}^{T - 1} x^t = \frac{1}{T} \sum_{t = 0}^{T - 1} x^t$ \newline 

Then for every $T \geq 1$ \newline 

$\mathbb{E}[f(\bar{x}^T) - \inf f] \leq \frac{||x^{(0)} - x^*||^2}{\gamma T} + 2\gamma \sigma_b^* $ \newline 

\noindent \textbf{Proof.} The proof of this is very simple as we did the majority of heavylifting in the last proof. We know that $\sum_{t = 0}^{T - 1} \gamma_t = \gamma T$ and $\sum_{t = 0}^{T - 1} \gamma_t^2 = T\gamma^2$. Let us substitute this into the last theorem and we will proceed from there. 

$\mathbb{E} [f(\bar{x}^T) - \inf f] \leq \frac{||x^{(0)} - x^*||^2}{\sum_{t = 1}^{T - 1} \gamma_t} + \frac{\sum_{t=1}^{T - 1} 2 \gamma_t^2 \sigma_b^*}{\sum_{t = 1}^{T - 1} \gamma_t}$ \newline

$\mathbb{E} [f(\bar{x}^T) - \inf f] \leq \frac{||x^{(0)} - x^*||^2}{\gamma T} + \frac{2 \sigma_b^* T\gamma^2}{\gamma T}$ \newline

$\mathbb{E} [f(\bar{x}^T) - \inf f] \leq \frac{||x^{(0)} - x^*||^2}{\gamma T} + 2 \sigma_b^* \gamma$ \newline

\noindent \textbf{Theorem.} Let us say that we have a function $f$ that is both a Sum of $L-$Smooth Functions and a Sum of Convex Functions. Let us say that the sequence of iterates generated by the SGD Algorithm is $(x^{(t)})_{t \in \mathbb{N}}$ with a vanishing step size $\gamma_t = \frac{\gamma_0}{\sqrt{t + 1}}$ where $\gamma_0 \leq \frac{1}{4L_{b}}$ 

Let us denote $\bar{x}^T = \frac{1}{\sum_{t = 0}^{T - 1} \gamma_t} \sum_{t = 0}^{T - 1} \gamma_t x^t$ \newline 

Then for every $T \geq 1$ \newline 

$\mathbb{E}[f(\bar{x}^T) - \inf f] \leq \frac{5 ||x^{(0)} - x^*||^2}{4 \gamma_0 \sqrt{T}} + \sigma_b^* \frac{5 \gamma_0 \log{(T + 1)}}{\sqrt{T}} = \mathbb{O}(\frac{\log{(T + 1)}}{\sqrt{T}})$ \newline 

\noindent \textbf{Proof.} We know that our stepsize is decreasing. Hence, we can clearly see that $\gamma_t \leq \gamma_0 \leq \frac{1}{4L_{b}}$ for $t \geq 0$. Hence, we can apply the earlier result that we derived: \newline 

$\mathbb{E}[f(\bar{x}^T) - \inf f] \leq \frac{||x^{(0)} - x^*||^2}{\sum_{t = 0}^{T - 1} \gamma_t} + 2\sigma_b^* \frac{\sum_{t = 0}^{T - 1} \gamma_t^2}{\sum_{t = 0}^{T - 1} \gamma_t}$ \newline 

Based on the Sum-Integral Bounds, we can say that $\sum_{t = 0}^{T - 1} \gamma_t = \gamma_0 \sum_{t = 1}^{T} \frac{1}{\sqrt{t}} \geq \frac{4\gamma_0}{5} \sqrt{T}$ \newline 

$\sum_{t = 0}^{T - 1} \gamma_t^2 = \gamma_0^2 \sum_{t = 1}^{T} \frac{1}{t} \leq 2 \gamma_0^2 \log(T + 1)$ \newline

Substituting it into the expected value, we get: \newline 

$\mathbb{E}[f(\bar{x}^T) - \inf f] \leq \frac{5 ||x^{(0)} - x^*||^2}{4 \gamma_0 \sqrt{T}} + \sigma_b^* \frac{5 \gamma_0 \log{(T + 1)}}{\sqrt{T}} = \mathbb{O}(\frac{\log{(T + 1)}}{\sqrt{T}})$ \newline 

\subsection{Minibatch SGD Convergence for Strongly Convex and Smooth Functions}
\noindent \textbf{Theorem.} Let us say that we have a function $f$ that is both a Sum of $L-$Smooth Functions and a Sum of Convex Functions. We will also assume that $f$ is $p$ strongly convex. Let us say that the sequence of iterates generated by the Minibatch SGD Algorithm is $(x^{(t)})_{t \in \mathbb{N}}$ and we will assume that we have a constant stepsize satisfying $0 < \gamma < \frac{1}{2L_{b}}$. Then, we can say that for each iteration(i.e. $t \geq 0$) 
\begin{equation}
    \mathbb{E}||x^{(t)} - x^*||^2 \leq (1 - \gamma p)^t ||x^{(0)} - x^*||^2 + \frac{2 \gamma}{p} \sigma_b^*
\end{equation}

\noindent \textbf{Proof:} \newline 
We know that, in Minibatch Stochastic Gradient Descent, our iterates progress as such: $x^{(t + 1)}  = x^{(t)} - \gamma \nabla f_{B_t}(x^{(t)})$ \newline 

$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - \gamma \nabla f_{B_t}(x^{(t - 1)}) - x^*||^2$ \newline 

$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^* - \gamma \nabla f_{B_t}(x^{(t - 1)})||^2$ \newline 


$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\langle x^{(t - 1)} - x^*,  \gamma \nabla f_{B_t}(x^{(t - 1)})\rangle + ||\gamma \nabla f_{B_t}(x^{(t - 1)})||^2$

$||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\langle x^{(t - 1)} - x^*,  \gamma \nabla f_{B_t}(x^{(t - 1)})\rangle + \gamma^2 ||\nabla f_{B_t}(x^{(t - 1)})||^2$

Now, let's take the Expectation conditioned on $x^{(t - 1)}$ \newline 
$\mathbb{E}||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\gamma \langle x^{(t - 1)} - x^*,  \nabla f(x^{(t - 1)})\rangle + \gamma^2 \mathbb{E} ||\nabla f_{B_t}(x^{(t - 1)})||^2$ \newline 


Based on the definition of strong convexity, we know that $f(y) \geq f(x) + \nabla(f(x))^T (y - x) + \frac{p}{2} ||y - x||^2_2$ \newline 


This would mean that $f(x^*) \geq f(x^{(t - 1)}) + \nabla(f(x^{(t - 1)}))^T (x^* - x^{(t - 1)}) + \frac{p}{2} ||x^* - x^{(t - 1)}||^2_2$ \newline 


$f(x^*) \geq f(x^{(t - 1)}) + \nabla(f(x^{(t - 1)}))^T (x^* - x^{(t - 1)}) + \frac{p}{2} ||x^* - x^{(t - 1)}||^2_2$ \newline 


$\nabla(f(x^{(t - 1)}))^T (x^{(t - 1)} - x^*) \geq f(x^{(t - 1)}) - f(x^*) + \frac{p}{2} ||x^{(t - 1)} - x^*||^2_2$ \newline 

We can substitute this into the earlier equation we derived and get: \newline 

$\mathbb{E}||x^{(t)} - x^*||^2 = ||x^{(t - 1)} - x^*||^2 - 2\gamma \langle x^{(t - 1)} - x^*,  \nabla f(x^{(t - 1)})\rangle + \gamma^2 \mathbb{E} ||\nabla f_{B_t}(x^{(t - 1)})||^2 \leq ||x^{(t - 1)} - x^*||^2 - 2\gamma (f(x^{(t - 1)}) - f(x^*) + \frac{p}{2} ||x^{(t - 1)} - x^*||^2_2) + \gamma^2 \mathbb{E} ||\nabla f_{B_t}(x^{(t - 1)})||^2$ \newline 

Let's try to simplify $||x^{(t - 1)} - x^*||^2 - 2\gamma (f(x^{(t - 1)}) - f(x^*) + \frac{p}{2} ||x^{(t - 1)} - x^*||^2_2) + \gamma^2 \mathbb{E} ||\nabla f_{B_t}(x^{(t - 1)})||^2$ \newline 

$||x^{(t - 1)} - x^*||^2 - 2\gamma (f(x^{(t - 1)}) - f(x^*)) - p \gamma ||x^{(t - 1)} - x^*||^2_2 + \gamma^2 \mathbb{E} ||\nabla f_{B_t}(x^{(t - 1)})||^2$ \newline 


$(1 - p \gamma)||x^{(t - 1)} - x^*||^2 - 2\gamma (f(x^{(t - 1)}) - f(x^*))+ \gamma^2 \mathbb{E} ||\nabla f_{B_t}(x^{(t - 1)})||^2$ \newline 

$\mathbb{E}||x^{(t)} - x^*||^2  \leq (1 - p \gamma)||x^{(t - 1)} - x^*||^2 - 2\gamma (f(x^{(t - 1)}) - f(x^*))+ \gamma^2 \mathbb{E} ||\nabla f_{B_t}(x^{(t - 1)})||^2$ \newline 

Earlier, we proved that, when we have a function that is a sum of $L-$ Smooth functions and that is a sum of convex functions, $\mathbb{E}[||\nabla f_B(x)||^2] \leq 4L_{b} (f(x) - \inf f) + 2 \sigma_b^*$ \newline 

We can continue on with our proof as such: \newline 

$\mathbb{E}||x^{(t)} - x^*||^2  \leq (1 - p \gamma) \mathbb{E}||x^{(t - 1)} - x^*||^2 - 2\gamma(f(x^{(t - 1)}) - f(x^*)) + \gamma^2 (4L_{b} (f(x^{(t - 1)}) - \inf f) + 2 \sigma_b^*)$ \newline 

$\mathbb{E}||x^{(t)} - x^*||^2  \leq (1 - p \gamma) \mathbb{E}||x^{(t - 1)} - x^*||^2 + (2 \gamma) (2 \gamma L_{b} - 1)(f(x^{(t - 1)}) - f(x^*)) + 2\gamma^2 \sigma_b^*$ \newline 

$\mathbb{E}||x^{(t)} - x^*||^2  \leq (1 - p \gamma) \mathbb{E}||x^{(t - 1)} - x^*||^2 + (2 \gamma) (2 \gamma L_{b} - 1) \mathbb{E}(f(x^{(t - 1)}) - f(x^*)) + 2\gamma^2 \sigma_b^*$ \newline 

Since $\gamma < \frac{1}{2L_{b}}$, $2\gamma L_{b} - 1 < 0$. We also know that $\mathbb{E}(f(x^{(t - 1)}) - f(x^*)) > 0$ Hence \newline 

$\mathbb{E}||x^{(t)} - x^*||^2  \leq (1 - p \gamma) \mathbb{E}||x^{(t - 1)} - x^*||^2 + (2 \gamma) (2 \gamma L_{b} - 1) \mathbb{E}(f(x^{(t - 1)}) - f(x^*)) + 2\gamma^2 \sigma_b^* \leq (1 - p \gamma) \mathbb{E}||x^{(t - 1)} - x^*||^2 + 2\gamma^2 \sigma_b^*$ \newline 


$\mathbb{E}||x^{(t)} - x^*||^2  \leq (1 - p \gamma) \mathbb{E}||x^{(t - 1)} - x^*||^2 + 2\gamma^2 \sigma_b^*$ \newline 

Let's build this inequality recursively and see how it unfolds: \newline 
$\mathbb{E}||x^{(1)} - x^*||^2  \leq (1 - p \gamma) ||x^{(0)} - x^*||^2 + 2\gamma^2 \sigma_b^*$ \newline 

$\mathbb{E}||x^{(2)} - x^*||^2  \leq (1 - p \gamma) \mathbb{E}||x^{(1)} - x^*||^2 + 2\gamma^2 \sigma_b^*$ \newline 

$\mathbb{E}||x^{(2)} - x^*||^2  \leq (1 - p \gamma) ((1 - p \gamma) ||x^{(0)} - x^*||^2 + 2\gamma^2 \sigma_b^*) + 2\gamma^2 \sigma_b^*$ \newline 


We can see that $\mathbb{E}||x^{(t)} - x^*||^2 \leq (1 - \gamma p)^t ||x^{(0)} - x^*||^2 + \sum_{n=0}^{t - 1} (1 - p\gamma)^n 2\gamma^2 \sigma_b^*$

Let's look at the term $\sum_{n=0}^{t - 1} (1 - p\gamma)^n 2\gamma^2 \sigma_b^*$. 

$\sum_{n=0}^{t - 1} (1 - p\gamma)^n 2\gamma^2 \sigma_b^* < \sum_{n=0}^{\infty} (1 - p\gamma)^n 2\gamma^2 \sigma_b^* = \frac{1}{p\gamma} 2\gamma^2 \sigma_b^* = \frac{2\gamma \sigma_b^*}{p}$

Hence, we can see that \newline 
$\mathbb{E}||x^{(t)} - x^*||^2 \leq (1 - \gamma p)^t ||x^{(0)} - x^*||^2 + \sum_{n=0}^{t - 1} (1 - p\gamma)^n 2\gamma^2 \sigma_b^* \leq (1 - \gamma p)^t ||x^{(0)} - x^*||^2 + \frac{2 \gamma}{p} \sigma_b^*$

$\mathbb{E}||x^{(t)} - x^*||^2 \leq (1 - \gamma p)^t ||x^{(0)} - x^*||^2 + \frac{2 \gamma}{p} \sigma_b^*$
