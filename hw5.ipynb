{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #5 - Ravi Raghavan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = - \\sum_{i=1}^{m} \\log{(1 - a_i^Tx)} - \\sum_{i=1}^{n} \\log{(1 - x_i^2)}$\n",
    "\n",
    "$x \\in \\mathbb{R}^n$ and $dom f = \\{ x | a_i^Tx < 1, i = 1, 2, ..., m, |x_i| < 1, i = 1, 2, ..., n\\}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial{f}}{\\partial{x_j}} = - (\\sum_{i=1}^{m} (\\frac{1}{1 - a_i^Tx}) (-a_{ij})) - \\frac{1}{(1 - x_j)^2} (-2x_j)$\n",
    "\n",
    "$\\frac{\\partial{f}}{\\partial{x_j}} =  \\sum_{i=1}^{m} (\\frac{a_{ij}}{1 - a_i^Tx}) + \\frac{2x_j}{(1 - x_j)^2}$\n",
    "\n",
    "\n",
    "$\\frac{\\partial{f}}{\\partial{x_k} \\partial{x_j}}$: \n",
    "\n",
    "Case where $j = k$:\n",
    "\n",
    "$\\frac{\\partial{f}}{\\partial{x_k} \\partial{x_j}} = \\sum_{i=1}^{m} a_{ij} (\\frac{-1}{(1 - a_i^Tx)^2}) (-a_{ik}) + \\frac{2(1 - x_j)^2 - 2x_j(-2(1 - x_j))}{(1 - x_j)^4}$\n",
    "\n",
    "$\\frac{\\partial{f}}{\\partial{x_k} \\partial{x_j}} = \\sum_{i=1}^{m} (\\frac{a_{ij} a_{ik}}{(1 - a_i^Tx)^2})  + \\frac{2(1 - x_j^2)}{(1 - x_j)^4}$\n",
    "\n",
    "$\\frac{\\partial{f}}{\\partial{x_k} \\partial{x_j}} = \\sum_{i=1}^{m} (\\frac{a_{ij} a_{ik}}{(1 - a_i^Tx)^2})  + \\frac{2(1 + x_j)}{(1 - x_j)^3}$\n",
    "\n",
    "\n",
    "Case where $j \\neq k$: \n",
    "\n",
    "$\\frac{\\partial{f}}{\\partial{x_k} \\partial{x_j}} = \\sum_{i=1}^{m} (\\frac{a_{ij} a_{ik}}{(1 - a_i^Tx)^2})$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, gradient, backtracking_algorithm, A: np.ndarray, x0: np.ndarray, max_iter, eta):\n",
    "    x = x0\n",
    "    fx = f(A, x)\n",
    "    \n",
    "    #maintain arrays to store iterates and function values throughout gradient descent\n",
    "    points = np.array([x])\n",
    "    function_values = []\n",
    "    function_values.append(fx)\n",
    "    \n",
    "    #enter for loop of max_iter times\n",
    "    for iter in range(max_iter):\n",
    "        grad = gradient(A, x) #compute gradient\n",
    "        descent_direction = -1 * grad #our descent direction is the negative gradient\n",
    "        alpha = backtracking_algorithm(A, x, descent_direction) #compute best step size\n",
    "        x = x + (alpha * descent_direction) #compute next iterate\n",
    "        \n",
    "        points = np.append(points, x[np.newaxis, :, :], axis=0) #store point in points array\n",
    "        fx = f(A, x) #calculate updated function value\n",
    "        function_values.append(fx) #store updated function value\n",
    "        \n",
    "        #if we have satisfied our convergence criteria, break from loop\n",
    "        if np.linalg.norm(grad) <= eta:\n",
    "            break\n",
    "    \n",
    "    #store function value in array\n",
    "    function_values = np.array(function_values)\n",
    "    return points, function_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(A: np.ndarray, x: np.ndarray):\n",
    "    B = A @ x\n",
    "    log_comp_B = np.log(1 - B)\n",
    "    log_x = np.log(1 - (np.square(x)))\n",
    "    return (-1 * np.sum(log_comp_B)) - np.sum(log_x)\n",
    "\n",
    "#each a_i is a row of A\n",
    "def f_gradient(A: np.ndarray, x: np.ndarray):\n",
    "    B = A @ x\n",
    "    gradient_vector = np.zeros(shape = x.shape)\n",
    "    \n",
    "    for j in range(x.shape[0]):\n",
    "        cum_total = 0\n",
    "        for i in range(A.shape[0]):\n",
    "            cum_total += A[i, j] / (1 - B[i, 0])\n",
    "        gradient_vector[j, 0] = cum_total + ((2 * x[j, 0]) / ((1 - x[j, 0]) ** 2))\n",
    "    \n",
    "    return gradient_vector   \n",
    "\n",
    "def f_backtracking_algorithm(A: np.ndarray, x: np.ndarray, delta_x: np.ndarray, alpha = 0.25, beta = 0.9):\n",
    "    t = 1\n",
    "    while (f(A, x + (t * delta_x))) > (f(A, x) + (alpha * t * (f_gradient(A, x).T @ delta_x))):\n",
    "        t = beta * t\n",
    "        \n",
    "    return t "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
